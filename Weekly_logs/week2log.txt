WEEK 2 LOG
Date: 11/16/2025 

Dataset Composition: 
- Utilized datasets containing 1,000 real face images and 1,000 AI-generated face images to train the model effectively.

Model Training:
- Documentation Utilization: Referenced original PyTorch documentation for guidance on training the model effectively.
- Hardware Upgrade: Changed from CPU to GPU to significantly speed up the training process.


Training Results:
- Initial Outcomes: Achieved an unrealistically high accuracy of 100% on the training dataset.
- This suggests that the dataset generated by AI is too uniform and lacks diversity.
- Indicated a potential issue with overfitting due to the lack of variability in the dataset.


Enhancing Diversity:
- Successfully sourced an additional 3,141 AI-generated and 3,141 real face images from various datasets on Kaggle, significantly enhancing the diversity of the training dataset.



Planning next steps:

Advanced Model Architectures:
- Experiment with more complex model architectures such as ResNet, EfficientNet, or Inception for potentially better feature extraction and classification performance.
- Consider implementing a Generative Adversarial Network (GAN) to generate synthetic images that can help improve the training dataset's diversity.


Regularization Techniques:
- Apply regularization methods such as Dropout or L2 regularization to combat overfitting and improve the model's generalization capabilities.
- Use techniques like early stopping to monitor validation accuracy and prevent overfitting during training.


Cross-Validation:
- Implement k-fold cross-validation to assess the model's performance more robustly and ensure that the model is not simply memorizing the training data.


Performance Metrics:
- Beyond accuracy, focus on additional metrics like precision, recall, and F1-score to better understand the model's performance, especially in classifying AI-generated vs. real faces.

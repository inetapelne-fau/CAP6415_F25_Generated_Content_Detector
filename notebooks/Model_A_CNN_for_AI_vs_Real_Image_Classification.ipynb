{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0u1E5JZLTd5"
      },
      "source": [
        "# **Generated Content Detector Project: AI generated photos vs real photos**\n",
        "# **Pytorch CNN Model**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuVEMKm1jFj2"
      },
      "source": [
        "## **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aDPk5LrLP1t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import random\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import classification_report\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ex_sm4L9XX0"
      },
      "source": [
        "## **Downloading the dataset**\n",
        "###### The !gdown command downloads a specific file from Google Drive directly into the current working directory of the Jupyter notebook, using its unique file ID.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q1xldHl-CsZ"
      },
      "outputs": [],
      "source": [
        "!gdown 1u4xb45DdfP80kxJ20DV20qAGyhcDaCDL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DlmVsgHP2Fz"
      },
      "source": [
        "###### The !unzip command extracts the contents of the ZIP file above, decompressing the files into the current directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "y2XiF8c5-MxU"
      },
      "outputs": [],
      "source": [
        "!unzip /content/AI-face-detection-Dataset-v3.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Cu3Yiuaqo8"
      },
      "source": [
        "## **Determining Optimal System Device for Performance: CPU vs. CUDA (VRAM)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOoO9TtsZFD5"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TFb-dZ3Jf6Q"
      },
      "source": [
        "## **Preparing Image Datasets with PyTorch.**\n",
        "##### **- Training Data: 7,000 images (3500 AI + 3500 real) - 70%**\n",
        "##### **- Testing Data: 2,000 images (1000 AI + 1000 real) - 20%**\n",
        "##### **- Testing Data: 1,000 images (500 AI + 500 real) - 10%**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEsPZLZlJImm"
      },
      "outputs": [],
      "source": [
        "# Measure the time taken for the cell execution:\n",
        "%%time\n",
        "\n",
        "# Set up image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the dataset from the specified directory\n",
        "dataset = datasets.ImageFolder(\"/content/AI-face-detection-Dataset-v3/\", transform=transform)\n",
        "\n",
        "# Load previously saved indices or datasets if available:\n",
        "try:\n",
        "    with open('datasets.pkl', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "        train_indices = data['train_indices']\n",
        "        val_indices = data['val_indices']\n",
        "        test_indices = data['test_indices']\n",
        "        classes = data['classes']\n",
        "except FileNotFoundError:\n",
        "    print(\"No saved dataset found, proceeding with the current run.\")\n",
        "\n",
        "# If loading fails, do the following steps:\n",
        "\n",
        "# Split by class (real + ai separately)\n",
        "indices_real = [i for i, (_, label) in enumerate(dataset) if label == dataset.class_to_idx[\"real_color\"]]\n",
        "indices_ai   = [i for i, (_, label) in enumerate(dataset) if label == dataset.class_to_idx[\"AI\"]]\n",
        "\n",
        "# Train size 70%, Validation size 10%, Test size 20%\n",
        "def split_class(indices, train_ratio=0.7, val_ratio=0.1):\n",
        "    train_len = int(len(indices) * train_ratio)\n",
        "    val_len = int(len(indices) * val_ratio)\n",
        "    return indices[:train_len], indices[train_len:train_len + val_len], indices[train_len + val_len:]\n",
        "\n",
        "train_real, val_real, test_real = split_class(indices_real)\n",
        "train_ai, val_ai, test_ai = split_class(indices_ai)\n",
        "\n",
        "# Combine training, validation, and testing indices\n",
        "train_indices = train_real + train_ai\n",
        "val_indices = val_real + val_ai\n",
        "test_indices = test_real + test_ai\n",
        "\n",
        "# Prepare subsets for training, validation, and testing\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "val_dataset = Subset(dataset, val_indices)\n",
        "test_dataset = Subset(dataset, test_indices)\n",
        "\n",
        "# Create data loaders for batch processing\n",
        "trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=2)  # Validation loader\n",
        "testloader = DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "# Retrieve class names\n",
        "classes = dataset.classes   # ['ai', 'real']\n",
        "\n",
        "# Output the number of samples in training, validation, and testing datasets\n",
        "print(\"Train samples:\", len(train_indices))\n",
        "print(\"Validation samples:\", len(val_indices))\n",
        "print(\"Test samples:\", len(test_indices))\n",
        "print(\"Classes:\", classes)\n",
        "\n",
        "# Save the datasets and indices:\n",
        "with open('datasets.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'train_indices': train_indices,\n",
        "        'val_indices': val_indices,\n",
        "        'test_indices': test_indices,\n",
        "        'classes': classes\n",
        "    }, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-61bEDyrbhF6"
      },
      "source": [
        "## **Displaying Randomly Selected Images with Classifications from the Trained Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFGex0rzMigY"
      },
      "outputs": [],
      "source": [
        "# Functions to display an image\n",
        "batch_size = 8\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize the image\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Fetch some random training images\n",
        "dataiter_train = iter(trainloader)\n",
        "images_train, labels_train = next(dataiter_train)\n",
        "\n",
        "# Display the training images\n",
        "imshow(torchvision.utils.make_grid(images_train))\n",
        "\n",
        "# Print the corresponding training labels\n",
        "print('Train Labels: ' + ' '.join(f'{classes[labels_train[j]]:5s}' for j in range(batch_size)))\n",
        "\n",
        "# Fetch some random validation images\n",
        "dataiter_val = iter(valloader)\n",
        "images_val, labels_val = next(dataiter_val)\n",
        "\n",
        "# Display the validation images\n",
        "imshow(torchvision.utils.make_grid(images_val))\n",
        "\n",
        "# Print the corresponding validation labels\n",
        "print('Validation Labels: ' + ' '.join(f'{classes[labels_val[j]]:5s}' for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4If1h4VZNq8S"
      },
      "source": [
        "## **Defining a Convolutional Neural Network (CNN) for Image Classification**\n",
        "##### This code defines a convolutional neural network (CNN) class named Net, designed for binary image classification. It comprises two convolutional layers with ReLU activation and max-pooling, followed by three fully connected layers. The forward method models the forward pass, sequentially applying these layers to transform input images into class predictions. This setup effectively extracts features and classifies the data, and the network can be instantiated for CPU or moved to GPU for faster computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D_ZNwcnwhZz"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5) # First convolutional layer\n",
        "        self.pool = nn.MaxPool2d(2, 2) # Max pooling layer\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5) # Second convolutional layer\n",
        "        self.fc1 = nn.Linear(16 * 29 * 29, 120)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(120, 84) # Second fully connected layer\n",
        "        self.fc3 = nn.Linear(84, 2)  # Output layer for 2 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x))) # Apply first conv layer and pooling\n",
        "        x = self.pool(F.relu(self.conv2(x))) # Apply second conv layer and pooling\n",
        "        x = torch.flatten(x, 1) # Flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x)) # Forward pass through first fully connected layer\n",
        "        x = F.relu(self.fc2(x)) # Forward pass through second fully connected layer\n",
        "        x = self.fc3(x) # Output layer\n",
        "        return x\n",
        "\n",
        "# net = Net()             # Use this for CPU\n",
        "net = Net().to(device)    # Move the model to GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M_gA8E_OE_4"
      },
      "source": [
        "## **Defining a Loss function and optimizer**\n",
        "###### I chose Stochastic Gradient Descent (SGD) over Adam because it often provides better generalization for certain datasets and allows for clearer hyperparameter tuning. Its simplicity leads to more stable convergence, particularly with larger batch sizes, making it a suitable choice for my classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOJDb8JsOEcT"
      },
      "outputs": [],
      "source": [
        "# Define the loss function for training\n",
        "criterion = nn.CrossEntropyLoss() # This is used for multi-class classification\n",
        "\n",
        "# Set up the optimizer for updating the model's parameters\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # Stochastic Gradient Descent with momentum\n",
        "\n",
        "# Alternative optimizer: Adam optimizer for adaptive learning rate\n",
        "# optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afgfBJnzOhsr"
      },
      "source": [
        "## **Training the Neural Network**\n",
        "##### An early stopping mechanism is implemented based on validation loss to mitigate overfitting. This technique terminates training when performance on the validation set starts to decline, preventing excessive fitting to the training data and enhancing overall model generalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b00LhdtfaTOQ"
      },
      "outputs": [],
      "source": [
        "# Measure the time taken for the cell execution:\n",
        "%%time\n",
        "\n",
        "train_losses = [] # Initialize a list to store training losses for each epoch\n",
        "val_losses = []  # List to store validation losses\n",
        "best_val_loss = float(\"inf\") # Initialize best validation loss to infinity\n",
        "patience = 8 # Number of epochs to wait before early stopping\n",
        "epochs_no_improve = 0 # Counter for epochs without improvement in validation loss\n",
        "\n",
        "# Loop through a specified number of epochs\n",
        "for epoch in range(50):\n",
        "    running_loss = 0.0  # Initialize cumulative loss for the current epoch\n",
        "\n",
        "    # Iterate through the training data\n",
        "    for i, data in enumerate(trainloader): # Iterate over the number of epochs\n",
        "        inputs, labels = data  # Get the input images and their corresponding labels\n",
        "\n",
        "        inputs = inputs.to(device)  # Move inputs to the device (GPU or CPU)\n",
        "        labels = labels.to(device)  # Move labels to the device\n",
        "\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "\n",
        "        outputs = net(inputs)  # Forward pass: compute the model's predictions\n",
        "\n",
        "        loss = criterion(outputs, labels)  # Compute the training loss\n",
        "\n",
        "        loss.backward()  # Backward pass: compute gradients\n",
        "        optimizer.step()  # Update model parameters\n",
        "\n",
        "        running_loss += loss.item()  # Accumulate the training loss\n",
        "\n",
        "    epoch_loss = running_loss / len(trainloader)  # Average loss for the epoch\n",
        "    train_losses.append(epoch_loss)  # Store the epoch loss\n",
        "    print(f\"Epoch {epoch+1}, Training loss: {epoch_loss:.4f}\")  # Print the training loss for the current epoch\n",
        "\n",
        "    # Validation phase\n",
        "    net.eval()  # Set the model to evaluation mode\n",
        "    val_running_loss = 0.0  # Initialize cumulative validation loss\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for validation\n",
        "        for data in valloader:\n",
        "            inputs, labels = data  # Get the validation images and labels\n",
        "            inputs = inputs.to(device)  # Move inputs to the device\n",
        "            labels = labels.to(device)  # Move labels to the device\n",
        "\n",
        "            outputs = net(inputs)  # Forward pass: compute predictions\n",
        "            loss = criterion(outputs, labels)  # Compute validation loss\n",
        "            val_running_loss += loss.item()  # Accumulate validation loss\n",
        "\n",
        "    val_epoch_loss = val_running_loss / len(valloader)  # Average validation loss for the epoch\n",
        "    val_losses.append(val_epoch_loss)  # Store the validation loss\n",
        "    print(f\"Epoch {epoch+1}, Validation loss: {val_epoch_loss:.4f}\")  # Print validation loss\n",
        "\n",
        "    if val_epoch_loss < best_val_loss: # If the current validation loss is the best\n",
        "        best_val_loss = val_epoch_loss # Update best validation loss\n",
        "        epochs_no_improve = 0 # Reset the no-improve counter\n",
        "        torch.save(net.state_dict(), \"best_model.pth\") # Save the best model\n",
        "    else: # If the validation loss did not improve\n",
        "        epochs_no_improve += 1 # Increment the no-improve counter\n",
        "        if epochs_no_improve >= patience: # Check if patience limit is reached\n",
        "            print(\"Early stopping\") # Indicate early stopping condition met\n",
        "            break  # Exit the training loop if no improvement\n",
        "\n",
        "print(\"Finished Training\")  # Indicate the training process is complete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7smA8OkgO9e"
      },
      "source": [
        "## **Learning Curve**\n",
        "##### Visualizing Training Results: The training loss graph demonstrates that the model has effectively learned from the training data, as indicated by the steady decline in training loss. The validation loss shows a similar downward trend initially but stabilizes afterward, suggesting good performance on unseen data. The implementation of early stopping has effectively prevented overfitting, allowing the model to maintain strong validation performance without excessive tuning to the training dataset. Overall, the close alignment between the training and validation loss lines reflects the model's good generalization capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpsCPQTn5jUI"
      },
      "outputs": [],
      "source": [
        "def plot_history(train_losses, val_losses):\n",
        "    plt.figure()  # Create a new figure for the plot\n",
        "    plt.plot(train_losses, label=\"train_loss\")  # Plot the training losses\n",
        "    plt.plot(val_losses, label=\"val_loss\")      # Plot the validation losses\n",
        "    plt.xlabel(\"Epoch\")                         # Label for the x-axis\n",
        "    plt.ylabel(\"Loss\")                          # Label for the y-axis\n",
        "    plt.title(\"Training and Validation Loss\")   # Title of the plot\n",
        "    plt.grid(True)                              # Add a grid for better readability\n",
        "    plt.legend()                                # Show the legend on the plot\n",
        "    plt.show()                                  # Display the plot\n",
        "\n",
        "# Check if 'train_losses' and 'val_losses' exist and are not empty before plotting\n",
        "if 'train_losses' in globals() and train_losses and 'val_losses' in globals() and val_losses:\n",
        "    plot_history(train_losses, val_losses)  # Call the plotting function with training and validation losses\n",
        "else:\n",
        "    print(\"Error: 'train_losses' or 'val_losses' not found or is empty. Please run the training cell first.\")  # Error message if the lists are unavailable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Validation Accuracy**"
      ],
      "metadata": {
        "id": "vwBBt4f-xwJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best saved model\n",
        "best_model = Net().to(device)\n",
        "best_model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in valloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = best_model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "final_val_accuracy = correct / total\n",
        "print(f\"Final Validation Accuracy (best model): {final_val_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "UeM_ju9XxxEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### This high validation accuracy indicates that the model has effectively learned from the training data and is performing well on unseen data, suggesting strong generalization capabilities without signs of overfitting."
      ],
      "metadata": {
        "id": "2AOwuciiH3ig"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HKGO3yvVoKd"
      },
      "source": [
        "## **Saving the Trained Model**\n",
        "\n",
        "#### The cell below requires your input to SAVE THE MODEL.\n",
        "#### Input Requirements: Please enter your DESIRED DIRECTORY in the designated section below this code. The window for input will show up once you run this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81PlaFnIVozg"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR DESIRED DIRECTORY TO SAVE THE MODEL\n",
        "# Please enter your input in the designated section below this code. The output will show up once you run this cell.\n",
        "save_directory = input(\"Enter the directory to save the model:\")\n",
        "\n",
        "# Local path on my Google Colab:\n",
        "# save_directory = '/content/drive/MyDrive/Colab_Notebooks/Generated_photo_detector_cnn_final_version'\n",
        "\n",
        "# Define the file name for the saved model\n",
        "file_name = 'ai_real_net_v7.pth'\n",
        "\n",
        "# Create the full path for saving the model\n",
        "PATH = os.path.join(save_directory, file_name)\n",
        "\n",
        "# Create the parent directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(PATH), exist_ok=True)\n",
        "\n",
        "# Save the model state dict to the specified path\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "# Print confirmation message with the model save location\n",
        "print(f'Model saved successfully at {PATH}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_TjNDcgp5oc"
      },
      "source": [
        "## **Loading the Saved Model and Summary of Architecture**\n",
        "##### The summary outlines the model's architecture, consisting of two convolutional layers—one with 456 parameters and the other without—followed by a max pooling layer containing 2,416 parameters. Three fully connected layers conclude the architecture, with the final layer having 170 parameters for class predictions. The model features approximately 1,628,046 trainable parameters, and its estimated total size is about 7.79 MB, indicating the computational resources needed for training and inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2RzA9QduCio"
      },
      "outputs": [],
      "source": [
        "# Initialize the neural network model\n",
        "net = Net()\n",
        "\n",
        "# Load the model's weights from the specified path\n",
        "net.load_state_dict(torch.load(PATH, map_location=device))  # Specify map_location for compatibility\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "net.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "net.eval()  # Important for layers like dropout and batch normalization if they are present\n",
        "\n",
        "# Display a summary of the model architecture with the corrected input size\n",
        "summary(net, (3, 128, 128))  # Display the model summary for input size of (3, 128, 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euxZpWXdV0rC"
      },
      "source": [
        "## **Testing the Network on Test Data**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14e4dfa2"
      },
      "source": [
        "#### Preparing and Predicting Test Images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b847055"
      },
      "source": [
        "net.eval() # Set the model to evaluation mode\n",
        "\n",
        "dataiter = iter(testloader)  # Create an iterator for the test data loader\n",
        "images, true_labels = next(dataiter)  # Get a batch of images and their true labels\n",
        "\n",
        "images = images.to(device)  # Move images to the appropriate device\n",
        "\n",
        "# Perform a forward pass and get predictions without calculating gradients\n",
        "with torch.no_grad():\n",
        "    outputs = net(images)\n",
        "\n",
        "_, predicted_labels = torch.max(outputs, 1) # Get the predicted class indices\n",
        "\n",
        "print(\"Retrieved a batch of test images and generated predictions.\")\n",
        "print(f\"Number of images in batch: {len(images)}\")\n",
        "print(f\"True labels (first 10): {true_labels[:10].tolist()}\")\n",
        "print(f\"Predicted labels (first 10): {predicted_labels[:10].tolist()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86a38584"
      },
      "source": [
        "#### Displaying Images with Actual and Predicted Labels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22b514b8"
      },
      "source": [
        "num_display = 10\n",
        "\n",
        "fig = plt.figure(figsize=(15, 6))\n",
        "\n",
        "for idx in range(num_display):\n",
        "    ax = fig.add_subplot(2, 5, idx + 1, xticks=[], yticks=[])\n",
        "\n",
        "    # Unnormalize the image and transpose dimensions\n",
        "    img_display = images[idx].cpu().numpy()\n",
        "    img_display = img_display / 2 + 0.5\n",
        "    img_display = np.transpose(img_display, (1, 2, 0))\n",
        "\n",
        "    # Display the image\n",
        "    ax.imshow(img_display)\n",
        "\n",
        "    # Set title with true and predicted labels\n",
        "    ax.set_title(f\"True: {classes[true_labels[idx]]}\\nPred: {classes[predicted_labels[idx]]}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24YCjlPzrVIa"
      },
      "source": [
        "## **Evaluating Network Performance on the Entire Dataset**\n",
        "### **Overall Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2APmVrqSrRI8"
      },
      "outputs": [],
      "source": [
        "correct = 0  # Initialize a counter for correctly classified images\n",
        "total = 0    # Initialize a counter for the total number of images\n",
        "\n",
        "# Disable gradient calculation for evaluation\n",
        "with torch.no_grad():\n",
        "    for data in testloader:  # Iterate through batches of test data\n",
        "        images, labels = data  # Get images and their corresponding labels\n",
        "\n",
        "        # Move inputs to the appropriate device for processing\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Calculate outputs by passing images through the network\n",
        "        outputs = net(images)\n",
        "\n",
        "        # Determine the predicted class by finding the index of the highest score\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)  # Update the total number of images processed\n",
        "        correct += (predicted == labels).sum().item()  # Count correct predictions\n",
        "\n",
        "# Calculate and print the accuracy of the network on the test dataset\n",
        "accuracy = 100 * correct / total if total > 0 else 0  # Prevent division by zero\n",
        "print(f'Accuracy of the network on the test images: {accuracy:.2f} %')  # Print accuracy formatted to two decimal places"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Classification Report**"
      ],
      "metadata": {
        "id": "6vvv_QdoTh2Y"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "876bfc91"
      },
      "source": [
        "# Ensure the model is in evaluation mode and on the correct device\n",
        "net.eval()\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "# Disable gradient calculation for efficiency during evaluation\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(all_labels, all_predictions, target_names=classes)\n",
        "\n",
        "# Print the report\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRcUd2_YkuA6"
      },
      "source": [
        "### **Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9529e048"
      },
      "outputs": [],
      "source": [
        "# Ensure the model is in evaluation mode and on the correct device\n",
        "net.eval()\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "# Disable gradient calculation for efficiency during evaluation\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Overall, the confusion matrix indicates that the model performs well, achieving high accuracy for both classes. The low false positive and false negative rates suggest that the model has good generalization capabilities and effectively distinguishes between AI-generated and real images."
      ],
      "metadata": {
        "id": "Mw6NA7hGkcs5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In conclusion, the combination of a strong final validation accuracy, high test accuracy, and favorable loss trends suggests that the model likely does not suffer from overfitting and can generalize effectively to new, unseen data."
      ],
      "metadata": {
        "id": "VRUFpJWYeRRw"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
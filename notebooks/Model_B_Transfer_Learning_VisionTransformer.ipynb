{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0u1E5JZLTd5"
      },
      "source": [
        "# **Generated Content Detector Project: AI generated photos vs real photos**\n",
        "# **Transfer Learning - Vision Transformer (ViT) Model**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuVEMKm1jFj2"
      },
      "source": [
        "## **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFRNcp15PFfe"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aDPk5LrLP1t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from torchinfo import summary\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pickle\n",
        "from typing import List, Tuple\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "from timeit import default_timer as timer\n",
        "import random\n",
        "from sklearn.metrics import classification_report\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ex_sm4L9XX0"
      },
      "source": [
        "## **Downloading the dataset**\n",
        "###### The !gdown command downloads a specific file from Google Drive directly into the current working directory of the Jupyter notebook, using its unique file ID.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q1xldHl-CsZ"
      },
      "outputs": [],
      "source": [
        "!gdown 1u4xb45DdfP80kxJ20DV20qAGyhcDaCDL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DlmVsgHP2Fz"
      },
      "source": [
        "###### The !unzip command extracts the contents of the ZIP file above, decompressing the files into the current directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "y2XiF8c5-MxU"
      },
      "outputs": [],
      "source": [
        "!unzip /content/AI-face-detection-Dataset-v3.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Cu3Yiuaqo8"
      },
      "source": [
        "## **Determining Optimal System Device for Performance: CPU vs. CUDA (VRAM)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOoO9TtsZFD5"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TFb-dZ3Jf6Q"
      },
      "source": [
        "## **Preparing Image Datasets with PyTorch.**\n",
        "##### **- Training Data: 7,000 images (3500 AI + 3500 real) - 70%**\n",
        "##### **- Testing Data: 2,000 images (1000 AI + 1000 real) - 20%**\n",
        "##### **- Testing Data: 1,000 images (500 AI + 500 real) - 10%**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEsPZLZlJImm"
      },
      "outputs": [],
      "source": [
        "# Measure the time taken for the cell execution:\n",
        "%%time\n",
        "\n",
        "# Set up image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the dataset from the specified directory\n",
        "dataset = datasets.ImageFolder(\"/content/AI-face-detection-Dataset-v3/\", transform=transform)\n",
        "\n",
        "# Load previously saved indices or datasets if available:\n",
        "try:\n",
        "    with open('datasets.pkl', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "        train_indices = data['train_indices']\n",
        "        val_indices = data['val_indices']\n",
        "        test_indices = data['test_indices']\n",
        "        classes = data['classes']\n",
        "except FileNotFoundError:\n",
        "    print(\"No saved dataset found, proceeding with the current run.\")\n",
        "\n",
        "# If loading fails, do the following steps:\n",
        "\n",
        "# Split by class (real + ai separately)\n",
        "indices_real = [i for i, (_, label) in enumerate(dataset) if label == dataset.class_to_idx[\"real_color\"]]\n",
        "indices_ai   = [i for i, (_, label) in enumerate(dataset) if label == dataset.class_to_idx[\"AI\"]]\n",
        "\n",
        "# Train size 70%, Validation size 10%, Test size 20%\n",
        "def split_class(indices, train_ratio=0.7, val_ratio=0.1):\n",
        "    train_len = int(len(indices) * train_ratio)\n",
        "    val_len = int(len(indices) * val_ratio)\n",
        "    return indices[:train_len], indices[train_len:train_len + val_len], indices[train_len + val_len:]\n",
        "\n",
        "train_real, val_real, test_real = split_class(indices_real)\n",
        "train_ai, val_ai, test_ai = split_class(indices_ai)\n",
        "\n",
        "# Combine training, validation, and testing indices\n",
        "train_indices = train_real + train_ai\n",
        "val_indices = val_real + val_ai\n",
        "test_indices = test_real + test_ai\n",
        "\n",
        "# Prepare subsets for training, validation, and testing\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "val_dataset = Subset(dataset, val_indices)\n",
        "test_dataset = Subset(dataset, test_indices)\n",
        "\n",
        "# Create data loaders for batch processing\n",
        "trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=2)  # Validation loader\n",
        "testloader = DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "# Retrieve class names\n",
        "classes = dataset.classes   # ['ai', 'real']\n",
        "\n",
        "# Output the number of samples in training, validation, and testing datasets\n",
        "print(\"Train samples:\", len(train_indices))\n",
        "print(\"Validation samples:\", len(val_indices))\n",
        "print(\"Test samples:\", len(test_indices))\n",
        "print(\"Classes:\", classes)\n",
        "\n",
        "# Save the datasets and indices:\n",
        "with open('datasets.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'train_indices': train_indices,\n",
        "        'val_indices': val_indices,\n",
        "        'test_indices': test_indices,\n",
        "        'classes': classes\n",
        "    }, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtQmJ8m_PQGD"
      },
      "source": [
        "## **Setting up a pretrained model - 'Vision Transformer (ViT) B-16'**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTGZbWMOPTJ7"
      },
      "outputs": [],
      "source": [
        "# Setup the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
        "weights = torchvision.models.ViT_B_16_Weights.DEFAULT # Selects the best available weights\n",
        "model = torchvision.models.vit_b_16(weights=weights).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvX5iiP6PQH6"
      },
      "source": [
        "## **Getting a summary of our model with torchinfo.summary()**\n",
        "###### The Vision Transformer (ViT) B-16 model is an advanced architecture for image classification and vision tasks, utilizing a transformer framework to process visual data. It divides images into 16x16 pixel patches and employs self-attention mechanisms to capture relationships between them. With approximately 86 million parameters, it balances complexity and efficiency, delivering high performance without excessive resource usage. The ViT B-16 excels in visual recognition, generalizing effectively across various datasets, and is ideal for applications such as image classification and object detection, marking a significant advancement in computer vision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvThgtXvPp5P"
      },
      "outputs": [],
      "source": [
        "summary(model=model,\n",
        "        input_size=(8, 3, 224, 224), # The size of the input tensor (batch size, channels, height, width)\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], # Specifies the columns to display in the summary\n",
        "        col_width=20, # Sets the width of the columns in the summary output\n",
        "        row_settings=[\"var_names\"]) # Includes variable names in the summary rows for clarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msHDRMM8PQJy"
      },
      "source": [
        "###### The Vision Transformer B-16 processes 224x224 pixel images in batches of 8, producing outputs for 1000 classes. It includes a convolutional layer that projects inputs to a 768-dimensional space, followed by 12 encoder blocks using self-attention mechanisms. A final linear layer outputs class probabilities. With 86,567,656 trainable parameters and approximately 1.39 billion multiply-add operations per forward pass, it exemplifies an effective balance of complexity and efficiency in image classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wg4W-cyPQP_"
      },
      "source": [
        "## **Freezing All Layers of the Base Model Except the Last Layer**\n",
        "###### Freezing all layers of the Vision Transformer model except the last layer during transfer learning helps retain the essential learned features from the pre-trained model, reducing the risk of overfitting, particularly with limited datasets. This strategy allows the model to focus on adapting the final layer, which is tailored to the new classification task, while preserving the foundational knowledge captured in the earlier layers. Additionally, it accelerates the training process and enhances stability in the early epochs, leading to more effective fine-tuning overall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEyjr_zYQJ5r"
      },
      "outputs": [],
      "source": [
        "# Freeze all parameters in the model first\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze the parameters of the classification head (the last layer)\n",
        "for param in model.heads.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze the last encoder layer\n",
        "num_encoder_layers = len(model.encoder.layers)\n",
        "for i in range(num_encoder_layers - 1, num_encoder_layers):\n",
        "    for param in model.encoder.layers[i].parameters():\n",
        "        param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjCCcyxKQJ8B"
      },
      "outputs": [],
      "source": [
        "# Set the manual seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Get the length of classes (one output unit for each class)\n",
        "output_shape = len(classes)\n",
        "\n",
        "# Recreate the classification head layer for VisionTransformer models\n",
        "# The classification head is typically model.heads.head\n",
        "model.heads.head = torch.nn.Sequential(\n",
        "    torch.nn.Dropout(p=0.2, inplace=True),\n",
        "    torch.nn.Linear(in_features=768, # The correct input features for ViT-B/16's head\n",
        "                    out_features=output_shape, # Same number of output units as our number of classes\n",
        "                    bias=True)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25Rde9SCQJ-i"
      },
      "outputs": [],
      "source": [
        "# # Do a summary *after* freezing the features and changing the output classifier layer\n",
        "summary(model,\n",
        "        input_size=(8, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
        "        verbose=0,\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iZK9XAFahJT"
      },
      "source": [
        "###### After freezing all but the last layer of the Vision Transformer model, the total parameters decreased from 86,567,656 to 85,800,194, reflecting a reduction in model complexity. The remaining 7,089,410 parameters are trainable, enabling adaptation to the specific task while preserving foundational knowledge. Despite this adjustment, memory usage and compute requirements for forward and backward passes remained similar, thereby enhancing training efficiency without compromising performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXl_eloePQSh"
      },
      "source": [
        "## **Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucbZ3v8VQ3gq"
      },
      "outputs": [],
      "source": [
        "# Define the loss function for training\n",
        "loss_fn = nn.CrossEntropyLoss() # This is used for multi-class classification\n",
        "\n",
        "# Set up the optimizer for updating the model's parameters\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) # Stochastic Gradient Descent with momentum\n",
        "\n",
        "# Alternative optimizer: Adam optimizer for adaptive learning rate\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXiUpXP1RCWH"
      },
      "outputs": [],
      "source": [
        "#ENGINE\n",
        "\"\"\"\n",
        "Contains functions for training and testing a PyTorch model.\n",
        "\"\"\"\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device) -> Tuple[float, float]:\n",
        "    \"\"\"Trains a PyTorch model for a single epoch.\n",
        "\n",
        "    Turns a target PyTorch model to training mode and then\n",
        "    runs through all of the required training steps (forward\n",
        "    pass, loss calculation, optimizer step).\n",
        "\n",
        "    Args:\n",
        "    model: A PyTorch model to be trained.\n",
        "    dataloader: A DataLoader instance for the model to be trained on.\n",
        "    loss_fn: A PyTorch loss function to minimize.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "    A tuple of training loss and training accuracy metrics.\n",
        "    In the form (train_loss, train_accuracy).\n",
        "    For example:(0.1112, 0.8743)\n",
        "    \"\"\"\n",
        "    # Put model in train mode to enable training features\n",
        "    model.train()\n",
        "\n",
        "    # Initialize train loss and train accuracy values\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    # Loop through batches of data from the dataloader\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # # Send data to the specified target device (GPU/CPU)\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 1. Forward pass: get predictions from the model\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # 2. Calculate and accumulate loss using the loss function\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item() # Accumulate loss for average calculation\n",
        "\n",
        "        # # 3. Optimizer zero grad: clear old gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward: compute gradients of loss with respect to model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. . Optimizer step: update model parameters based on gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate and accumulate accuracy metric across all batches\n",
        "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    train_loss = train_loss / len(dataloader) # Average loss over batches\n",
        "    train_acc = train_acc / len(dataloader) # Average accuracy over batches\n",
        "    return train_loss, train_acc\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device) -> Tuple[float, float]:\n",
        "    \"\"\"Tests a PyTorch model for a single epoch.\n",
        "\n",
        "    Turns a target PyTorch model to \"eval\" mode and then performs\n",
        "    a forward pass on a testing dataset.\n",
        "\n",
        "    Args:\n",
        "    model: A PyTorch model to be tested.\n",
        "    dataloader: A DataLoader instance for the model to be tested on.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "    A tuple of testing loss and testing accuracy metrics.\n",
        "    In the form (test_loss, test_accuracy).\n",
        "    For example: (0.0223, 0.8985)\n",
        "    \"\"\"\n",
        "    # Put model in eval mode to deactivate dropout layers and batch norm behavior\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize test loss and test accuracy values\n",
        "    test_loss, test_acc = 0, 0\n",
        "\n",
        "    # Turn on inference context manager to save memory during inference\n",
        "    with torch.inference_mode():\n",
        "        # Loop through DataLoader batches for testing\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            # Send data to the specified target device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # 1. Forward pass: get predictions from the model\n",
        "            test_pred_logits = model(X)\n",
        "\n",
        "            # 2. Calculate and accumulate loss\n",
        "            loss = loss_fn(test_pred_logits, y)\n",
        "            test_loss += loss.item() # Accumulate loss for average calculation\n",
        "\n",
        "            # Calculate and accumulate accuracy\n",
        "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    test_loss = test_loss / len(dataloader)\n",
        "    test_acc = test_acc / len(dataloader)\n",
        "    return test_loss, test_acc\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device) -> Dict[str, List]:\n",
        "    \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "    Passes a target PyTorch models through train_step() and test_step()\n",
        "    functions for a number of epochs, training and testing the model\n",
        "    in the same epoch loop.\n",
        "\n",
        "    Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "    Args:\n",
        "    model: A PyTorch model to be trained and tested.\n",
        "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: An integer indicating how many epochs to train for.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "    A dictionary of training and testing loss as well as training and\n",
        "    testing accuracy metrics. Each metric has a value in a list for\n",
        "    each epoch.\n",
        "    In the form: {train_loss: [...],\n",
        "              train_acc: [...],\n",
        "              test_loss: [...],\n",
        "              test_acc: [...]}\n",
        "    For example if training for epochs=2:\n",
        "             {train_loss: [2.0616, 1.0537],\n",
        "              train_acc: [0.3945, 0.3945],\n",
        "              test_loss: [1.2641, 1.5706],\n",
        "              test_acc: [0.3400, 0.2973]}\n",
        "    \"\"\"\n",
        "    # Create empty results dictionary\n",
        "    results = {\"train_loss\": [],\n",
        "               \"train_acc\": [],\n",
        "               \"test_loss\": [],\n",
        "               \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    # Make sure model on target device\n",
        "    model.to(device)\n",
        "\n",
        "    # Loop through training and testing steps for a number of epochs\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                          dataloader=train_dataloader,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          optimizer=optimizer,\n",
        "                                          device=device)\n",
        "        test_loss, test_acc = test_step(model=model,\n",
        "          dataloader=test_dataloader,\n",
        "          loss_fn=loss_fn,\n",
        "          device=device)\n",
        "\n",
        "        # Print out what's happening\n",
        "        print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "    # Return the filled results at the end of the epochs\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKIVkcxgRjVj"
      },
      "outputs": [],
      "source": [
        "# Set the random seeds for reproducibility in training\n",
        "torch.manual_seed(42)  # Set seed for CPU\n",
        "torch.cuda.manual_seed(42)  # Set seed for GPU\n",
        "\n",
        "# Start the timer to measure training duration\n",
        "start_time = timer()\n",
        "\n",
        "# Setup training and save the results\n",
        "results = train(model=model,\n",
        "                       train_dataloader=trainloader,  # DataLoader for training data\n",
        "                       test_dataloader=valloader, # DataLoader for validation/testing data\n",
        "                       optimizer=optimizer, # Optimizer for model training\n",
        "                       loss_fn=loss_fn, # Loss function to evaluate model performance\n",
        "                       epochs=10, # Number of training epochs\n",
        "                       device=device)\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh8HzbNZQ4Gc"
      },
      "source": [
        "## **Learning Curve**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2x6Y--zRvZ8"
      },
      "outputs": [],
      "source": [
        "# Plot loss curves of a model\n",
        "def plot_loss_curves(results):\n",
        "    \"\"\"Plots training curves of a results dictionary.\n",
        "\n",
        "    Args:\n",
        "        results (dict): dictionary containing list of values, e.g.\n",
        "            {\"train_loss\": [...],\n",
        "             \"train_acc\": [...],\n",
        "             \"test_loss\": [...],\n",
        "             \"test_acc\": [...]}\n",
        "    \"\"\"\n",
        "    loss = results[\"train_loss\"]\n",
        "    test_loss = results[\"test_loss\"]\n",
        "\n",
        "    accuracy = results[\"train_acc\"]\n",
        "    test_accuracy = results[\"test_acc\"]\n",
        "\n",
        "    epochs = range(len(results[\"train_loss\"]))\n",
        "\n",
        "    plt.figure(figsize=(15, 7))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, loss, label=\"train_loss\")\n",
        "    plt.plot(epochs, test_loss, label=\"test_loss\")\n",
        "    plt.title(\"Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, accuracy, label=\"train_accuracy\")\n",
        "    plt.plot(epochs, test_accuracy, label=\"test_accuracy\")\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZW5zySXR9IT"
      },
      "outputs": [],
      "source": [
        "# Plot the loss curves of our model\n",
        "plot_loss_curves(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amI894uce8BS"
      },
      "source": [
        "###### The learning curves indicate that while the training loss decreases steadily and training accuracy improves significantly, the test loss remains consistently higher than the training loss, suggesting potential overfitting. The test accuracy shows fluctuations, indicating sensitivity to the test data and possible generalization issues. To address these concerns, implementing regularization techniques, considering data augmentation, and using early stopping could help improve model performance and robustness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwBBt4f-xwJM"
      },
      "source": [
        "## **Validation Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeM_ju9XxxEA"
      },
      "outputs": [],
      "source": [
        "# Load the best saved model\n",
        "# The 'Net' class is not defined, using the 'model' variable from previous cells.\n",
        "best_model = model\n",
        "# The model is already loaded and trained, so no need to load state dict again.\n",
        "# best_model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in valloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = best_model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "final_val_accuracy = correct / total\n",
        "print(f\"Final Validation Accuracy (best model): {final_val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AOwuciiH3ig"
      },
      "source": [
        "###### My final validation accuracy is 0.9599, which shows that my model performs very well on unseen data. This high accuracy indicates effective learning and good generalization, making it a strong candidate for practical applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euxZpWXdV0rC"
      },
      "source": [
        "## **Testing the Network on Test Data**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14e4dfa2"
      },
      "source": [
        "#### Preparing and Predicting Test Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b847055"
      },
      "outputs": [],
      "source": [
        "model.eval() # Set the model to evaluation mode\n",
        "\n",
        "dataiter = iter(testloader)  # Create an iterator for the test data loader\n",
        "images, true_labels = next(dataiter)  # Get a batch of images and their true labels\n",
        "\n",
        "images = images.to(device)  # Move images to the appropriate device\n",
        "\n",
        "# Perform a forward pass and get predictions without calculating gradients\n",
        "with torch.no_grad():\n",
        "    outputs = model(images)\n",
        "\n",
        "_, predicted_labels = torch.max(outputs, 1) # Get the predicted class indices\n",
        "\n",
        "print(\"Retrieved a batch of test images and generated predictions.\")\n",
        "print(f\"Number of images in batch: {len(images)}\")\n",
        "print(f\"True labels (first 10): {true_labels[:10].tolist()}\")\n",
        "print(f\"Predicted labels (first 10): {predicted_labels[:10].tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86a38584"
      },
      "source": [
        "#### Displaying Images with Actual and Predicted Labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22b514b8"
      },
      "outputs": [],
      "source": [
        "num_display = 10\n",
        "\n",
        "fig = plt.figure(figsize=(15, 6))\n",
        "\n",
        "for idx in range(num_display):\n",
        "    ax = fig.add_subplot(2, 5, idx + 1, xticks=[], yticks=[])\n",
        "\n",
        "    # Unnormalize the image and transpose dimensions\n",
        "    img_display = images[idx].cpu().numpy()\n",
        "    img_display = img_display / 2 + 0.5\n",
        "    img_display = np.transpose(img_display, (1, 2, 0))\n",
        "\n",
        "    # Display the image\n",
        "    ax.imshow(img_display)\n",
        "\n",
        "    # Set title with true and predicted labels\n",
        "    ax.set_title(f\"True: {classes[true_labels[idx]]}\\nPred: {classes[predicted_labels[idx]]}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24YCjlPzrVIa"
      },
      "source": [
        "## **Evaluating Network Performance on the Entire Dataset**\n",
        "### **Overall Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2APmVrqSrRI8"
      },
      "outputs": [],
      "source": [
        "correct = 0  # Initialize a counter for correctly classified images\n",
        "total = 0    # Initialize a counter for the total number of images\n",
        "\n",
        "# Disable gradient calculation for evaluation\n",
        "with torch.no_grad():\n",
        "    for data in testloader:  # Iterate through batches of test data\n",
        "        images, labels = data  # Get images and their corresponding labels\n",
        "\n",
        "        # Move inputs to the appropriate device for processing\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Calculate outputs by passing images through the network\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Determine the predicted class by finding the index of the highest score\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)  # Update the total number of images processed\n",
        "        correct += (predicted == labels).sum().item()  # Count correct predictions\n",
        "\n",
        "# Calculate and print the accuracy of the network on the test dataset\n",
        "accuracy = 100 * correct / total if total > 0 else 0  # Prevent division by zero\n",
        "print(f'Accuracy of the network on the test images: {accuracy:.2f} %')  # Print accuracy formatted to two decimal places"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCqB2pLif0Lz"
      },
      "source": [
        "###### The accuracy of the network on the test images is 96.95%, demonstrating that the model generalizes well to new data and effectively captures the underlying patterns in the dataset. This strong performance reinforces the model's reliability for practical use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vvv_QdoTh2Y"
      },
      "source": [
        "### **Classification Report**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "876bfc91"
      },
      "outputs": [],
      "source": [
        "# Ensure the model is in evaluation mode and on the correct device\n",
        "model.eval()\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "# Disable gradient calculation for efficiency during evaluation\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(all_labels, all_predictions, target_names=classes)\n",
        "\n",
        "# Print the report\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-kW1CRDgXVD"
      },
      "source": [
        "###### The model demonstrates strong performance with an overall accuracy of 97% on the test set of 2002 samples. Both classes, AI and Real, exhibit high precision (0.96 and 0.98, respectively) and recall (0.98 for AI and 0.96 for Real Color), resulting in balanced F1-scores of 0.97 for both. The macro and weighted averages further reinforce the model's reliability, indicating consistent effectiveness in distinguishing between the two classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRcUd2_YkuA6"
      },
      "source": [
        "### **Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9529e048"
      },
      "outputs": [],
      "source": [
        "# Ensure the model is in evaluation mode and on the correct device\n",
        "model.eval()\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "# Disable gradient calculation for efficiency during evaluation\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw6NA7hGkcs5"
      },
      "source": [
        "###### The confusion matrix indicates strong model performance, with 984 true positives for the AI class and 957 for the real class. There are 44 false negatives, where real_color was misclassified as AI, and 17 false positives, where AI was incorrectly labeled as real_color. Overall, the model effectively distinguishes between the two categories but could benefit from improvements in reducing false negatives for the real class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRUFpJWYeRRw"
      },
      "source": [
        "#### **Summary:** The model exhibits impressive performance across various metrics, achieving an overall accuracy of 97% on the test set of 2002 samples. Precision and recall are high, with the AI class scoring 0.96 and 0.98, respectively, while the real_color class scores 0.98 and 0.96. Both classes have an F1-score of 0.97, indicating a balanced performance. The confusion matrix further supports these results, showing 984 true positives for AI and 957 for real_color, with 44 false negatives and 17 false positives. Overall, the model demonstrates strong classification capabilities but could improve in reducing false negatives for the real_color class."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}